
# üöÄ Datalake Serverless para An√°lise de Dados de Streaming do Mercado de A√ß√µes

> **Resumo:**
> Este projeto demonstra como construir um datalake totalmente serverless na AWS para ingest√£o, processamento e an√°lise de dados de streaming em tempo real do mercado de a√ß√µes, utilizando dados da API da Finnhub.

---

## üìä Vis√£o Geral

O objetivo √© criar uma arquitetura escal√°vel, sem servidor, capaz de processar grandes volumes de transa√ß√µes de a√ß√µes em tempo real. Os dados s√£o coletados da Finnhub, enviados para o Amazon Kinesis Data Streams, processados por uma fun√ß√£o AWS Lambda e armazenados no Amazon DynamoDB para an√°lise e consulta.

---

## üèóÔ∏è Arquitetura

| Servi√ßo AWS         | Fun√ß√£o no Projeto                                                                 |
|---------------------|----------------------------------------------------------------------------------|
| **Kinesis Data Streams** | Ingest√£o de dados em tempo real (entrada dos dados do Finnhub)                  |
| **AWS Lambda**          | Processamento dos dados recebidos do Kinesis                                    |
| **DynamoDB**            | Armazenamento dos dados processados para consulta/an√°lise                       |
| **IAM**                 | Gerenciamento de permiss√µes e roles para seguran√ßa dos recursos                 |

**Fluxo de Dados:**

```
Finnhub WebSocket API ‚Üí producer.py ‚Üí Amazon Kinesis Data Streams ‚Üí AWS Lambda ‚Üí Amazon DynamoDB
```

---

## üõ†Ô∏è Tecnologias Utilizadas

| Tecnologia      | Descri√ß√£o                                      |
|-----------------|------------------------------------------------|
| Python          | Linguagem principal dos scripts                 |
| Boto3           | SDK AWS para Python                             |
| Finnhub API     | Fonte dos dados de mercado em tempo real        |
| AWS CLI         | Gerenciamento e deploy dos recursos AWS         |
| Poetry          | Gerenciamento de depend√™ncias Python            |

---

## üìÅ Componentes do Projeto

| Arquivo/Ferramenta      | Fun√ß√£o                                                                 |
|-------------------------|------------------------------------------------------------------------|
| `producer.py`           | Conecta √† Finnhub, recebe dados e envia para o Kinesis                  |
| `consumer.py`           | L√≥gica do consumidor (Lambda) para processar dados do Kinesis           |
| `pyproject.toml`/`poetry.lock` | Gerenciamento de depend√™ncias Python                        |

---

## ‚öôÔ∏è Como Configurar e Executar

### ‚úÖ Pr√©-requisitos

| Requisito | Descri√ß√£o |
|-----------|-----------|
| Conta AWS | Permiss√µes para criar/gerenciar Kinesis, Lambda, DynamoDB e IAM |
| Python 3.8+ | Instalado |
| Poetry | Instalado |
| AWS CLI | Configurado |
| Chave Finnhub | [Obtenha aqui](https://finnhub.io/) |

---

### üìù Passos

1. **Clonar o reposit√≥rio:**
   ```bash
   git clone <url-do-repositorio>
   cd <nome-do-repositorio>
   ```

2. **Instalar depend√™ncias:**
   ```bash
   poetry install
   ```

3. **Configurar Vari√°veis de Ambiente:**
   Crie um arquivo `.env` na raiz do projeto com:
   ```env
   FINNHUB_API_KEY="SUA_CHAVE_API_FINNHUB"
   SECRET_KEY="SUA_CHAVE_SECRETA_SE_NECESSARIO"
   ```

4. **Configura√ß√£o dos Recursos na AWS:**
   <details>
   <summary><strong>Etapas detalhadas</strong></summary>

   #### a. üñ•Ô∏è Configura√ß√£o do AWS CLI
   Execute no terminal e preencha as informa√ß√µes solicitadas:
   ```bash
   aws configure
   ```

   #### b. üîÑ Cria√ß√£o do Kinesis Data Stream
   1. Acesse o console do **Amazon Kinesis**.
   2. No menu lateral, clique em **Data Streams**.
   3. Clique em **Criar fluxo de dados**.
   4. Nome do fluxo: `datalake-stream` (deve ser igual ao do `producer.py`).
   5. Modo de capacidade: **Sob demanda**.
   6. Clique em **Criar fluxo de dados**.

   #### c. üóÑÔ∏è Cria√ß√£o da Tabela no DynamoDB
   1. Console do **Amazon DynamoDB**.
   2. Clique em **Criar tabela**.
   3. Nome: `stock_trades`.
   4. Chave de parti√ß√£o: `s` (s√≠mbolo da a√ß√£o).
   5. Chave de classifica√ß√£o: `t` (timestamp da transa√ß√£o).
   6. Clique em **Criar tabela**.

   #### d. üîê Configura√ß√£o de Permiss√µes no IAM
   | Role/Pol√≠tica | Descri√ß√£o |
   |---------------|-----------|
   | **AWSLambdaKinesisExecutionRole** | Permite que a Lambda leia dados do Kinesis |
   | **Pol√≠tica customizada DynamoDB** | Permite gravar dados no DynamoDB usando `BatchWriteItem` |

   **Exemplo de pol√≠tica customizada para DynamoDB:**
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": ["dynamodb:BatchWriteItem"],
         "Resource": "arn:aws:dynamodb:<sua-regiao>:<seu-id-de-conta>:table/stock_trades"
       }
     ]
   }
   ```

   **Como associar as roles √† Lambda:**
   1. No console do **IAM**, crie uma nova role para Lambda.
   2. Anexe a pol√≠tica gerenciada **AWSLambdaKinesisExecutionRole**.
   3. Crie e anexe a pol√≠tica customizada acima para o DynamoDB.

   #### e. ‚ö° Cria√ß√£o da Fun√ß√£o Lambda
   1. Console do **AWS Lambda**.
   2. Clique em **Criar fun√ß√£o** > **Criar do zero**.
   3. Nome: `KinesisToDynamoDBProcessor`.
   4. Runtime: Python 3.9+.
   5. Permiss√µes: Selecione a role criada acima.
   6. Clique em **Criar fun√ß√£o**.
   7. Adicione o gatilho do Kinesis (`datalake-stream`).
   8. Cole o c√≥digo do `consumer.py` e clique em **Deploy**.

   </details>

5. **Executar o Produtor:**
   Atualize o nome do `KINESIS_STREAM_NAME` no `producer.py` e execute:
   ```bash
   poetry run python producer.py
   ```

---

## üìà Observa√ß√£o

O terminal exibir√° os dados recebidos do Finnhub e enviados ao Kinesis. Voc√™ pode acompanhar os logs da Lambda e os dados inseridos no DynamoDB pelo console AWS.

---

## üí° Relev√¢ncia

Este projeto exemplifica uma arquitetura moderna, escal√°vel e de baixo custo para ingest√£o e processamento de dados em tempo real, utilizando apenas servi√ßos gerenciados da AWS. √â ideal para aplica√ß√µes que exigem an√°lise imediata de grandes volumes de dados, como no setor financeiro, IoT e monitoramento de sistemas.
